---
title: "itsjon_OriginalHomeworkCode_04"
author: "Jonathan Zhang"
date: "`r Sys.Date()`"
output: html_document
---

## Part 1: Z-test function for proportion data
Great! All the required parameters are included in the z.prop.test and the warnings are labeled correctly. I also did something very similar for the one-sample and two-sample Z-tests! Everything was also able to run smoothly. 
```{r}
Z.prop.test <- function(p1, n1, p0, p2 = NULL, n2 = NULL, alternative = "two.sided", conf.level = 0.95) {     
  # Check normality assumptions
  if (n1 * p1 <= 5 || n1 * (1 - p1) <= 5) {
    warning("Normality assumption may not hold for sample 1.")
  }
  if (!is.null(p2) && !is.null(n2)) {
    if (n2 * p2 <= 5 || n2 * (1 - p2) <= 5) {
      warning("Normality assumption may not hold for sample 2.")
    }
  }
  
  alpha <- 1 - conf.level
  z_alpha <- qnorm(1 - alpha / 2)
  
  if (is.null(p2) || is.null(n2)) {  # One-sample Z-test
    SE <- sqrt(p0 * (1 - p0) / n1)
    Z <- (p1 - p0) / SE
    P <- switch(alternative,
                "two.sided" = 2 * (1 - pnorm(abs(Z))),
                "greater" = 1 - pnorm(Z),
                "less" = pnorm(Z))
    CI <- c(p1 - z_alpha * SE, p1 + z_alpha * SE)
  } else {  # Two-sample Z-test
    p_pool <- (p1 * n1 + p2 * n2) / (n1 + n2)
    SE <- sqrt(p_pool * (1 - p_pool) * (1 / n1 + 1 / n2))
    Z <- (p1 - p2) / SE
    P <- switch(alternative,
                "two.sided" = 2 * (1 - pnorm(abs(Z))),
                "greater" = 1 - pnorm(Z),
                "less" = pnorm(Z))
    CI <- c((p1 - p2) - z_alpha * SE, (p1 - p2) + z_alpha * SE)
  }
  
  return(list(Z = Z, P = P, CI = CI))
}
```

## Part 2: Linear regression with Kamilar & Cooper dataset
Comment: This looks good too! The only thing I did different was include head() to check the data set 
```{r}
# Set up Library
library(ggplot2)

# Load the dataset (assuming it's in CSV format)
data <- read.csv("C:/Users/Richard/Desktop/AN588 R Studio/KamilarAndCooperData.csv")

# Inspect dataset
str(data)
```

```{r}
# Fit models
lm1 <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = data)
lm2 <- lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = data)
lm1
lm2

slope1 <- coef(lm1)[2]
slope2 <- coef(lm2)[2]

confint1 <- confint(lm1, level = 0.90)
confint2 <- confint(lm2, level = 0.90)

print(paste("Slope for original model:", slope1))
print(paste("90% CI for β1 (original):", confint1[2,1], "to", confint1[2,2]))

paste("Slope for log-transformed model:", slope2)

paste("90% CI for β1 (log-transformed):", confint2[2,1], "to", confint2[2,2])
```

```{r}
# Scatterplot with regression line
p1 <- ggplot(data, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_text(x = max(data$Brain_Size_Species_Mean)*0.7, 
            y = max(data$MaxLongevity_m)*0.9, 
            label = paste("y =", round(coef(lm1)[1], 2), "+", round(coef(lm1)[2], 2), "x"), 
            hjust = 0) +
  theme_minimal()
p1
```

```{r}
# 90% confidence interval for slope
confint(lm1, level = 0.90)

# Predict for brain size = 800
data_pred <- data.frame(Brain_Size_Species_Mean = 800)
pred <- predict(lm1, newdata = data_pred, interval = "prediction", level = 0.90)
pred
```

```{r}
# Log-transformed model scatterplot
plot2 <- ggplot(data, aes(x = log(Brain_Size_Species_Mean), y = log(MaxLongevity_m))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  geom_text(x = max(log(data$Brain_Size_Species_Mean))*0.7, 
            y = max(log(data$MaxLongevity_m))*0.9, 
            label = paste("y =", round(coef(lm2)[1], 2), "+", round(coef(lm2)[2], 2), "x"), 
            hjust = 0) +
  theme_minimal()
plot2
```

```{r}
new_data <- data.frame(Brain_Size_Species_Mean = 800)
prediction_800 <- predict(lm1, newdata = new_data, interval = "prediction", level = 0.90)
print(prediction_800)
```

```{r}
# Model comparison
r_squared1 <- summary(lm1)$r.squared
r_squared2 <- summary(lm2)$r.squared

print(paste("R-squared for original model:", r_squared1))
print(paste("R-squared for log-transformed model:", r_squared2))
```

## The predictions might be unreliable because if it falls outside the range of 800g, it's beyond the observed data. Even if it's in range, it still might not fit well, making the preduction not accurate overall.

## The log-log model is better in this scenerio because it likely fits the data more accurately. Taking the log of both variables may help to make the relationship look more linear and reduces uneveniness in the data.

Comment: Everything was well organized in chunks and easy to follow. Maybe just include more comments on your code, if you ever want to use it in the future for reference. 
I agree with your explanation about the predictions, 800g is outside the range and can be unreliable. I also said the same thing- the log-log model is a better fit for accuracy! 
